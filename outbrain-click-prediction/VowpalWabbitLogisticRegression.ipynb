{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession\n",
    "\n",
    "https://spark.apache.org/docs/2.4.4/api/python/pyspark.html\n",
    "\n",
    "https://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [      
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import spark_utils\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"yarn\", \"My App\", conf=spark_utils.get_spark_conf())\n",
    "se = SparkSession(sc)\n",
    "spark_utils.print_ui_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register all tables for sql queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * clicked\n",
    "#  * - geo_location features (country, state, dma)\n",
    "#  * - day_of_week (from timestamp, use date.isoweekday())\n",
    "# * ad_id\n",
    "# * campaign_id\n",
    "# * advertiser_id\n",
    "#  * ad_document_id\n",
    "# * display_document_id\n",
    "# * platform\n",
    "\n",
    "se.sql(\"\"\"\n",
    "select \n",
    "    date_format(( cast(current_timestamp as TIMESTAMP) + INTERVAL 1465876799 seconds), \"u\") as week_of_day,        \n",
    "    split(e.geo_location, '>')[0] as country,\n",
    "    CASE WHEN size(split(e.geo_location, '>')) >= 2 THEN split(e.geo_location, '>')[1] \n",
    "    ELSE ''\n",
    "    END as state,\n",
    "    CASE WHEN size(split(e.geo_location, '>')) >= 3 THEN split(e.geo_location, '>')[2] \n",
    "    ELSE ''\n",
    "    END as dma,\n",
    "    CONCAT(pc.ad_id, '_', pc.document_id) AS ad_document_id,\n",
    "    CONCAT(ct.display_id, '_', pc.document_id) AS display_document_id,\n",
    "    ct.clicked,\n",
    "    ct.display_id,\n",
    "    ct.ad_id,\n",
    "    pc.document_id,\n",
    "    pc.campaign_id,\n",
    "    pc.advertiser_id,\n",
    "    e.platform #no platform\n",
    "from clicks_train ct, events e, promoted_content pc where\n",
    "ct.ad_id == pc.ad_id and e.display_id == ct.display_id\n",
    "\"\"\").write.parquet(\"/train_features_baseline.parquet\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. geo_location & platform (events) join with clicked on display_id\n",
    "2. put timestamp & display_id from events into new table; convert timestamp into day_of_week using python function; join with clicked on display_id\n",
    "3. put ad_document_id and ad_id from promoted_content into new table, where ad_document_id is the concatenation of 2 columns; join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916020dce0ef4198b745da2a485d498f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>display_id</th>\n",
       "      <th>ad_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16874594</td>\n",
       "      <td>66758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16874594</td>\n",
       "      <td>150083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16874594</td>\n",
       "      <td>162754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  display_id   ad_id\n",
       "0   16874594   66758\n",
       "1   16874594  150083\n",
       "2   16874594  162754"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_train\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>display_id</th>\n",
       "      <th>ad_id</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>42337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>139684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>144739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  display_id   ad_id clicked\n",
       "0          1   42337       0\n",
       "1          1  139684       0\n",
       "2          1  144739       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents_categories\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>confidence_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1595802</td>\n",
       "      <td>1611</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1595802</td>\n",
       "      <td>1610</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1524246</td>\n",
       "      <td>1807</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id category_id confidence_level\n",
       "0     1595802        1611             0.92\n",
       "1     1595802        1610             0.07\n",
       "2     1524246        1807             0.92"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents_entities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>confidence_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1524246</td>\n",
       "      <td>f9eec25663db4cd83183f5c805186f16</td>\n",
       "      <td>0.672865314504701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1524246</td>\n",
       "      <td>55ebcfbdaff1d6f60b3907151f38527a</td>\n",
       "      <td>0.399113728441297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1524246</td>\n",
       "      <td>839907a972930b17b125eb0247898412</td>\n",
       "      <td>0.392095749652966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id                         entity_id   confidence_level\n",
       "0     1524246  f9eec25663db4cd83183f5c805186f16  0.672865314504701\n",
       "1     1524246  55ebcfbdaff1d6f60b3907151f38527a  0.399113728441297\n",
       "2     1524246  839907a972930b17b125eb0247898412  0.392095749652966"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents_meta\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>publisher_id</th>\n",
       "      <th>publish_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1595802</td>\n",
       "      <td>1</td>\n",
       "      <td>603</td>\n",
       "      <td>2016-06-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1524246</td>\n",
       "      <td>1</td>\n",
       "      <td>603</td>\n",
       "      <td>2016-05-26 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1617787</td>\n",
       "      <td>1</td>\n",
       "      <td>603</td>\n",
       "      <td>2016-05-27 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id source_id publisher_id         publish_time\n",
       "0     1595802         1          603  2016-06-05 00:00:00\n",
       "1     1524246         1          603  2016-05-26 11:00:00\n",
       "2     1617787         1          603  2016-05-27 00:00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents_topics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>confidence_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1595802</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0731131601068925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1595802</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0594164867373976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1595802</td>\n",
       "      <td>143</td>\n",
       "      <td>0.0454207537554526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id topic_id    confidence_level\n",
       "0     1595802      140  0.0731131601068925\n",
       "1     1595802       16  0.0594164867373976\n",
       "2     1595802      143  0.0454207537554526"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>display_id</th>\n",
       "      <th>uuid</th>\n",
       "      <th>document_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>platform</th>\n",
       "      <th>geo_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>cb8c55702adb93</td>\n",
       "      <td>379743</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>US&gt;SC&gt;519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>79a85fa78311b9</td>\n",
       "      <td>1794259</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>US&gt;CA&gt;807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>822932ce3d8757</td>\n",
       "      <td>1179111</td>\n",
       "      <td>182</td>\n",
       "      <td>2</td>\n",
       "      <td>US&gt;MI&gt;505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  display_id            uuid document_id timestamp platform geo_location\n",
       "0          1  cb8c55702adb93      379743        61        3    US>SC>519\n",
       "1          2  79a85fa78311b9     1794259        81        2    US>CA>807\n",
       "2          3  822932ce3d8757     1179111       182        2    US>MI>505"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_views\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>document_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>platform</th>\n",
       "      <th>geo_location</th>\n",
       "      <th>traffic_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1fd5f051fba643</td>\n",
       "      <td>120</td>\n",
       "      <td>31905835</td>\n",
       "      <td>1</td>\n",
       "      <td>RS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8557aa9004be3b</td>\n",
       "      <td>120</td>\n",
       "      <td>32053104</td>\n",
       "      <td>1</td>\n",
       "      <td>VN&gt;44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c351b277a358f0</td>\n",
       "      <td>120</td>\n",
       "      <td>54013023</td>\n",
       "      <td>1</td>\n",
       "      <td>KR&gt;12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             uuid document_id timestamp platform geo_location traffic_source\n",
       "0  1fd5f051fba643         120  31905835        1           RS              2\n",
       "1  8557aa9004be3b         120  32053104        1        VN>44              2\n",
       "2  c351b277a358f0         120  54013023        1        KR>12              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_views_sample\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>document_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>platform</th>\n",
       "      <th>geo_location</th>\n",
       "      <th>traffic_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1fd5f051fba643</td>\n",
       "      <td>120</td>\n",
       "      <td>31905835</td>\n",
       "      <td>1</td>\n",
       "      <td>RS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8557aa9004be3b</td>\n",
       "      <td>120</td>\n",
       "      <td>32053104</td>\n",
       "      <td>1</td>\n",
       "      <td>VN&gt;44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c351b277a358f0</td>\n",
       "      <td>120</td>\n",
       "      <td>54013023</td>\n",
       "      <td>1</td>\n",
       "      <td>KR&gt;12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             uuid document_id timestamp platform geo_location traffic_source\n",
       "0  1fd5f051fba643         120  31905835        1           RS              2\n",
       "1  8557aa9004be3b         120  32053104        1        VN>44              2\n",
       "2  c351b277a358f0         120  54013023        1        KR>12              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promoted_content\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad_id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>advertiser_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6614</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>471467</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7692</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ad_id document_id campaign_id advertiser_id\n",
       "0     1        6614           1             7\n",
       "1     2      471467           2             7\n",
       "2     3        7692           3             7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "tables = [\"clicks_test\", \"clicks_train\", \n",
    "          \"documents_categories\", \"documents_entities\", \"documents_meta\", \"documents_topics\", \n",
    "          \"events\", \"page_views\", \"page_views_sample\", \"promoted_content\"]\n",
    "for name in tqdm.tqdm(tables):\n",
    "    df = se.read.parquet(\"s3://ydatazian/{}.parquet\".format(name))\n",
    "    df.registerTempTable(name)\n",
    "    print(name)\n",
    "    display(df.limit(3).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for VW\n",
    "\n",
    "We will predict a *click* based on:\n",
    "- ad_id\n",
    "- document_id\n",
    "- campaign_id\n",
    "- advertiser_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "se.sql(\"\"\"\n",
    "select \n",
    "    clicks_train.clicked,\n",
    "    clicks_train.display_id,\n",
    "    clicks_train.ad_id,\n",
    "    promoted_content.document_id,\n",
    "    promoted_content.campaign_id,\n",
    "    promoted_content.advertiser_id\n",
    "from clicks_train join promoted_content on clicks_train.ad_id = promoted_content.ad_id\n",
    "\"\"\").write.parquet(\"/train_features.parquet\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----+---+--------------+-------------------+-------+----------+------+-----------+-----------+-------------+--------+\n",
      "|week_of_day|country|state|dma|ad_document_id|display_document_id|clicked|display_id| ad_id|document_id|campaign_id|advertiser_id|platform|\n",
      "+-----------+-------+-----+---+--------------+-------------------+-------+----------+------+-----------+-----------+-------------+--------+\n",
      "|          3|     US|   CA|807|132815_1227645|   10000108_1227645|      1|  10000108|132815|    1227645|      17018|          331|       1|\n",
      "|          3|     US|   CA|807|133677_1297868|   10000108_1297868|      0|  10000108|133677|    1297868|      17143|         1919|       1|\n",
      "|          3|     US|   CA|807|406686_1671134|   10000108_1671134|      0|  10000108|406686|    1671134|      27505|         2879|       1|\n",
      "|          3|     US|   CA|807|406704_1678714|   10000108_1678714|      0|  10000108|406704|    1678714|      27506|         2879|       1|\n",
      "|          3|     US|   CA|807|449087_2135836|   10000108_2135836|      0|  10000108|449087|    2135836|      31474|         3052|       1|\n",
      "+-----------+-------+-----+---+--------------+-------------------+-------+----------+------+-----------+-----------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.read.parquet(\"/train_features_baseline.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10000108_132815| week_of_day_3 country_US state_CA dma_807 ad_document_id_132815_1227645 display_document_id_10000108_1227645 display_id_10000108 ad_id_132815 document_id_1227645 campaign_id_17018 advertiser_id_331 platform_1\n",
      "-1 10000108_133677| week_of_day_3 country_US state_CA dma_807 ad_document_id_133677_1297868 display_document_id_10000108_1297868 display_id_10000108 ad_id_133677 document_id_1297868 campaign_id_17143 advertiser_id_1919 platform_1\n"
     ]
    }
   ],
   "source": [
    "# Format: [Label] [Importance] [Base] [Tag]|Namespace Features |Namespace Features ... |Namespace Features\n",
    "# https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Input-format\n",
    "def vw_row_mapper(row):\n",
    "    clicked = None\n",
    "    features = []\n",
    "    for k, v in row.asDict().items():\n",
    "        if k == 'clicked':\n",
    "            clicked = '1' if v == '1' else '-1'\n",
    "        else:\n",
    "            features.append(k + \"_\" + str(v))\n",
    "    tag = row.display_id + \"_\" + row.ad_id\n",
    "    return \"{} {}| {}\".format(clicked, tag, \" \".join(features))\n",
    "\n",
    "r = se.read.parquet(\"/train_features_baseline.parquet\").take(2)\n",
    "#print(r)\n",
    "print(vw_row_mapper(r[0]))\n",
    "print(vw_row_mapper(r[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /train_features.txt\n",
      "CPU times: user 59.3 ms, sys: 24.5 ms, total: 83.7 ms\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! hdfs dfs -rm -r /train_features.txt\n",
    "(\n",
    "    se.read.parquet(\"/train_features_baseline.parquet\")\n",
    "    .rdd\n",
    "    .map(vw_row_mapper)\n",
    "    .saveAsTextFile(\"/train_features.txt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.2 G  /train_features.txt\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -du -s -h /train_features.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/mnt/train.txt': No such file or directory\n",
      "1 10000108_132815| week_of_day_3 country_US state_CA dma_807 ad_document_id_132815_1227645 display_document_id_10000108_1227645 display_id_10000108 ad_id_132815 document_id_1227645 campaign_id_17018 advertiser_id_331 platform_1\n",
      "-1 10000108_133677| week_of_day_3 country_US state_CA dma_807 ad_document_id_133677_1297868 display_document_id_10000108_1297868 display_id_10000108 ad_id_133677 document_id_1297868 campaign_id_17143 advertiser_id_1919 platform_1\n",
      "-1 10000108_406686| week_of_day_3 country_US state_CA dma_807 ad_document_id_406686_1671134 display_document_id_10000108_1671134 display_id_10000108 ad_id_406686 document_id_1671134 campaign_id_27505 advertiser_id_2879 platform_1\n",
      "-1 10000108_406704| week_of_day_3 country_US state_CA dma_807 ad_document_id_406704_1678714 display_document_id_10000108_1678714 display_id_10000108 ad_id_406704 document_id_1678714 campaign_id_27506 advertiser_id_2879 platform_1\n",
      "-1 10000108_449087| week_of_day_3 country_US state_CA dma_807 ad_document_id_449087_2135836 display_document_id_10000108_2135836 display_id_10000108 ad_id_449087 document_id_2135836 campaign_id_31474 advertiser_id_3052 platform_1\n",
      "CPU times: user 521 ms, sys: 139 ms, total: 660 ms\n",
      "Wall time: 47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# copy file to local master node\n",
    "! rm /mnt/train.txt\n",
    "! hdfs dfs -getmerge /train_features.txt /mnt/train.txt\n",
    "# preview local file\n",
    "! head -n 5 /mnt/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VW\n",
    "https://vowpalwabbit.org/tutorials/getting_started.html\n",
    "\n",
    "https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = model\n",
      "Enabling FTRL based optimization\n",
      "Algorithm used: Proximal-FTRL\n",
      "ftrl_alpha = 0.005\n",
      "ftrl_beta = 0.1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = /mnt/train.txt.cache\n",
      "Reading datafile = /mnt/train.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.459290 0.459290      8000000      8000000.0  -1.0000  -2.5621       13\n",
      "0.455174 0.451059     16000000     16000000.0   1.0000  -0.9695       13\n",
      "0.453008 0.448675     24000000     24000000.0   1.0000  -1.6506       13\n",
      "0.451665 0.447636     32000000     32000000.0  -1.0000  -1.5811       13\n",
      "0.450678 0.446732     40000000     40000000.0  -1.0000  -2.8322       13\n",
      "0.449896 0.445987     48000000     48000000.0  -1.0000  -1.0146       13\n",
      "0.449312 0.445807     56000000     56000000.0   1.0000  -1.0821       13\n",
      "0.448830 0.445453     64000000     64000000.0  -1.0000  -0.4269       13\n",
      "0.448423 0.445168     72000000     72000000.0  -1.0000  -2.7416       13\n",
      "0.448057 0.444764     80000000     80000000.0   1.0000  -1.9614       13\n",
      "\n",
      "finished run\n",
      "number of examples = 87141731\n",
      "weighted example sum = 87141731.000000\n",
      "weighted label sum = -53392545.000000\n",
      "average loss = 0.447772\n",
      "best constant = -1.426495\n",
      "best constant's loss = 0.491466\n",
      "total feature number = 1132842503\n"
     ]
    }
   ],
   "source": [
    "! ./vw -d /mnt/train.txt -b 24 -c -k --ftrl --passes 1 -f model --holdout_off --loss_function logistic --random_seed 42 --progress 8000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = \r\n",
      "num sources = 1\r\n",
      "driver:\r\n",
      "  --onethread           Disable parse thread\r\n",
      "VW options:\r\n",
      "  --ring_size arg (=256, ) size of example ring\r\n",
      "  --strict_parse           throw on malformed examples\r\n",
      "Update options:\r\n",
      "  -l [ --learning_rate ] arg Set learning rate\r\n",
      "  --power_t arg              t power value\r\n",
      "  --decay_learning_rate arg  Set Decay factor for learning_rate between passes\r\n",
      "  --initial_t arg            initial t value\r\n",
      "  --feature_mask arg         Use existing regressor to determine which \r\n",
      "                             parameters may be updated.  If no \r\n",
      "                             initial_regressor given, also used for initial \r\n",
      "                             weights.\r\n",
      "Weight options:\r\n",
      "  -i [ --initial_regressor ] arg  Initial regressor(s)\r\n",
      "  --initial_weight arg            Set all weights to an initial value of arg.\r\n",
      "  --random_weights                make initial weights random\r\n",
      "  --normal_weights                make initial weights normal\r\n",
      "  --truncated_normal_weights      make initial weights truncated normal\r\n",
      "  --sparse_weights                Use a sparse datastructure for weights\r\n",
      "  --input_feature_regularizer arg Per feature regularization input file\r\n",
      "Parallelization options:\r\n",
      "  --span_server arg                 Location of server for setting up spanning \r\n",
      "                                    tree\r\n",
      "  --unique_id arg (=0, )            unique id used for cluster parallel jobs\r\n",
      "  --total arg (=1, )                total number of nodes used in cluster \r\n",
      "                                    parallel job\r\n",
      "  --node arg (=0, )                 node number in cluster parallel job\r\n",
      "  --span_server_port arg (=26543, ) Port of the server for setting up spanning \r\n",
      "                                    tree\r\n",
      "Diagnostic options:\r\n",
      "  --version             Version information\r\n",
      "  -a [ --audit ]        print weights of features\r\n",
      "  -P [ --progress ] arg Progress update frequency. int: additive, float: \r\n",
      "                        multiplicative\r\n",
      "  --quiet               Don't output disgnostics and progress updates\r\n",
      "  -h [ --help ]         Look here: http://hunch.net/~vw/ and click on Tutorial.\r\n",
      "Randomization options:\r\n",
      "  --random_seed arg     seed random number generator\r\n",
      "Feature options:\r\n",
      "  --hash arg                      how to hash the features. Available options: \r\n",
      "                                  strings, all\r\n",
      "  --hash_seed arg (=0, )          seed for hash function\r\n",
      "  --ignore arg                    ignore namespaces beginning with character \r\n",
      "                                  <arg>\r\n",
      "  --ignore_linear arg             ignore namespaces beginning with character \r\n",
      "                                  <arg> for linear terms only\r\n",
      "  --keep arg                      keep namespaces beginning with character \r\n",
      "                                  <arg>\r\n",
      "  --redefine arg                  redefine namespaces beginning with characters\r\n",
      "                                  of string S as namespace N. <arg> shall be in\r\n",
      "                                  form 'N:=S' where := is operator. Empty N or \r\n",
      "                                  S are treated as default namespace. Use ':' \r\n",
      "                                  as a wildcard in S.\r\n",
      "  -b [ --bit_precision ] arg      number of bits in the feature table\r\n",
      "  --noconstant                    Don't add a constant feature\r\n",
      "  -C [ --constant ] arg           Set initial value of constant\r\n",
      "  --ngram arg                     Generate N grams. To generate N grams for a \r\n",
      "                                  single namespace 'foo', arg should be fN.\r\n",
      "  --skips arg                     Generate skips in N grams. This in \r\n",
      "                                  conjunction with the ngram tag can be used to\r\n",
      "                                  generate generalized n-skip-k-gram. To \r\n",
      "                                  generate n-skips for a single namespace \r\n",
      "                                  'foo', arg should be fN.\r\n",
      "  --feature_limit arg             limit to N features. To apply to a single \r\n",
      "                                  namespace 'foo', arg should be fN\r\n",
      "  --affix arg                     generate prefixes/suffixes of features; \r\n",
      "                                  argument '+2a,-3b,+1' means generate 2-char \r\n",
      "                                  prefixes for namespace a, 3-char suffixes for\r\n",
      "                                  b and 1 char prefixes for default namespace\r\n",
      "  --spelling arg                  compute spelling features for a give \r\n",
      "                                  namespace (use '_' for default namespace)\r\n",
      "  --dictionary arg                read a dictionary for additional features \r\n",
      "                                  (arg either 'x:file' or just 'file')\r\n",
      "  --dictionary_path arg           look in this directory for dictionaries; \r\n",
      "                                  defaults to current directory or env{PATH}\r\n",
      "  --interactions arg              Create feature interactions of any level \r\n",
      "                                  between namespaces.\r\n",
      "  --permutations                  Use permutations instead of combinations for \r\n",
      "                                  feature interactions of same namespace.\r\n",
      "  --leave_duplicate_interactions  Don't remove interactions with duplicate \r\n",
      "                                  combinations of namespaces. For ex. this is a\r\n",
      "                                  duplicate: '-q ab -q ba' and a lot more in \r\n",
      "                                  '-q ::'.\r\n",
      "  -q [ --quadratic ] arg          Create and use quadratic features\r\n",
      "  --q: arg                        : corresponds to a wildcard for all printable\r\n",
      "                                  characters\r\n",
      "  --cubic arg                     Create and use cubic features\r\n",
      "Example options:\r\n",
      "  -t [ --testonly ]                Ignore label information and just test\r\n",
      "  --holdout_off                    no holdout data in multiple passes\r\n",
      "  --holdout_period arg (=10, )     holdout period for test only\r\n",
      "  --holdout_after arg              holdout after n training examples, default \r\n",
      "                                   off (disables holdout_period)\r\n",
      "  --early_terminate arg (=3, )     Specify the number of passes tolerated when \r\n",
      "                                   holdout loss doesn't decrease before early \r\n",
      "                                   termination\r\n",
      "  --passes arg                     Number of Training Passes\r\n",
      "  --initial_pass_length arg        initial number of examples per pass\r\n",
      "  --examples arg                   number of examples to parse\r\n",
      "  --min_prediction arg             Smallest prediction to output\r\n",
      "  --max_prediction arg             Largest prediction to output\r\n",
      "  --sort_features                  turn this on to disregard order in which \r\n",
      "                                   features have been defined. This will lead \r\n",
      "                                   to smaller cache sizes\r\n",
      "  --loss_function arg (=squared, ) Specify the loss function to be used, uses \r\n",
      "                                   squared by default. Currently available ones\r\n",
      "                                   are squared, classic, hinge, logistic, \r\n",
      "                                   quantile and poisson.\r\n",
      "  --quantile_tau arg (=0.5, )      Parameter \\tau associated with Quantile \r\n",
      "                                   loss. Defaults to 0.5\r\n",
      "  --l1 arg                         l_1 lambda\r\n",
      "  --l2 arg                         l_2 lambda\r\n",
      "  --no_bias_regularization         no bias in regularization\r\n",
      "  --named_labels arg               use names for labels (multiclass, etc.) \r\n",
      "                                   rather than integers, argument specified all\r\n",
      "                                   possible labels, comma-sep, eg \r\n",
      "                                   \"--named_labels Noun,Verb,Adj,Punc\"\r\n",
      "Output model:\r\n",
      "  -f [ --final_regressor ] arg          Final regressor\r\n",
      "  --readable_model arg                  Output human-readable final regressor \r\n",
      "                                        with numeric features\r\n",
      "  --invert_hash arg                     Output human-readable final regressor \r\n",
      "                                        with feature names.  Computationally \r\n",
      "                                        expensive.\r\n",
      "  --save_resume                         save extra state so learning can be \r\n",
      "                                        resumed later with new data\r\n",
      "  --preserve_performance_counters       reset performance counters when \r\n",
      "                                        warmstarting\r\n",
      "  --save_per_pass                       Save the model after every pass over \r\n",
      "                                        data\r\n",
      "  --output_feature_regularizer_binary arg\r\n",
      "                                        Per feature regularization output file\r\n",
      "  --output_feature_regularizer_text arg Per feature regularization output file,\r\n",
      "                                        in text\r\n",
      "  --id arg                              User supplied ID embedded into the \r\n",
      "                                        final regressor\r\n",
      "Output options:\r\n",
      "  -p [ --predictions ] arg     File to output predictions to\r\n",
      "  -r [ --raw_predictions ] arg File to output unnormalized predictions to\r\n",
      "Audit Regressor:\r\n",
      "  --audit_regressor arg stores feature names and their regressor values. Same \r\n",
      "                        dataset must be used for both regressor training and \r\n",
      "                        this mode.\r\n",
      "Search options:\r\n",
      "  --search arg                          Use learning to search, \r\n",
      "                                        argument=maximum action id or 0 for LDF\r\n",
      "  --search_task arg                     the search task (use \"--search_task \r\n",
      "                                        list\" to get a list of available tasks)\r\n",
      "  --search_metatask arg                 the search metatask (use \r\n",
      "                                        \"--search_metatask list\" to get a list \r\n",
      "                                        of available metatasks)\r\n",
      "  --search_interpolation arg            at what level should interpolation \r\n",
      "                                        happen? [*data|policy]\r\n",
      "  --search_rollout arg                  how should rollouts be executed?       \r\n",
      "                                            [policy|oracle|*mix_per_state|mix_p\r\n",
      "                                        er_roll|none]\r\n",
      "  --search_rollin arg                   how should past trajectories be \r\n",
      "                                        generated? [policy|oracle|*mix_per_stat\r\n",
      "                                        e|mix_per_roll]\r\n",
      "  --search_passes_per_policy arg (=1, ) number of passes per policy (only valid\r\n",
      "                                        for search_interpolation=policy)\r\n",
      "  --search_beta arg (=0.5, )            interpolation rate for policies (only \r\n",
      "                                        valid for search_interpolation=policy)\r\n",
      "  --search_alpha arg (=1e-10, )         annealed beta = 1-(1-alpha)^t (only \r\n",
      "                                        valid for search_interpolation=data)\r\n",
      "  --search_total_nb_policies arg        if we are going to train the policies \r\n",
      "                                        through multiple separate calls to vw, \r\n",
      "                                        we need to specify this parameter and \r\n",
      "                                        tell vw how many policies are \r\n",
      "                                        eventually going to be trained\r\n",
      "  --search_trained_nb_policies arg      the number of trained policies in a \r\n",
      "                                        file\r\n",
      "  --search_allowed_transitions arg      read file of allowed transitions [def: \r\n",
      "                                        all transitions are allowed]\r\n",
      "  --search_subsample_time arg           instead of training at all timesteps, \r\n",
      "                                        use a subset. if value in (0,1), train \r\n",
      "                                        on a random v%. if v>=1, train on \r\n",
      "                                        precisely v steps per example, if \r\n",
      "                                        v<=-1, use active learning\r\n",
      "  --search_neighbor_features arg        copy features from neighboring lines. \r\n",
      "                                        argument looks like: '-1:a,+2' meaning \r\n",
      "                                        copy previous line namespace a and next\r\n",
      "                                        next line from namespace _unnamed_, \r\n",
      "                                        where ',' separates them\r\n",
      "  --search_rollout_num_steps arg        how many calls of \"loss\" before we stop\r\n",
      "                                        really predicting on rollouts and \r\n",
      "                                        switch to oracle (default means \r\n",
      "                                        \"infinite\")\r\n",
      "  --search_history_length arg (=1, )    some tasks allow you to specify how \r\n",
      "                                        much history their depend on; specify \r\n",
      "                                        that here\r\n",
      "  --search_no_caching                   turn off the built-in caching ability \r\n",
      "                                        (makes things slower, but technically \r\n",
      "                                        more safe)\r\n",
      "  --search_xv                           train two separate policies, \r\n",
      "                                        alternating prediction/learning\r\n",
      "  --search_perturb_oracle arg (=0, )    perturb the oracle on rollin with this \r\n",
      "                                        probability\r\n",
      "  --search_linear_ordering              insist on generating examples in linear\r\n",
      "                                        order (def: hoopla permutation)\r\n",
      "  --search_active_verify arg            verify that active learning is doing \r\n",
      "                                        the right thing (arg = multiplier, \r\n",
      "                                        should be = cost_range * range_c)\r\n",
      "  --search_save_every_k_runs arg        save model every k runs\r\n",
      "Experience Replay:\r\n",
      "  --replay_c arg              use experience replay at a specified level \r\n",
      "                              [b=classification/regression, m=multiclass, \r\n",
      "                              c=cost sensitive] with specified buffer size\r\n",
      "  --replay_c_count arg (=1, ) how many times (in expectation) should each \r\n",
      "                              example be played (default: 1 = permuting)\r\n",
      "Explore evaluation:\r\n",
      "  --explore_eval        Evaluate explore_eval adf policies\r\n",
      "  --multiplier arg      Multiplier used to make all rejection sample \r\n",
      "                        probabilities <= 1\r\n",
      "Make csoaa_ldf into Contextual Bandit:\r\n",
      "  --cbify_ldf           Convert csoaa_ldf into a contextual bandit problem\r\n",
      "  --loss0 arg (=0, )    loss for correct label\r\n",
      "  --loss1 arg (=1, )    loss for incorrect label\r\n",
      "Make Multiclass into Contextual Bandit:\r\n",
      "  --cbify arg           Convert multiclass on <k> classes into a contextual \r\n",
      "                        bandit problem\r\n",
      "  --cbify_cs            consume cost-sensitive classification examples instead \r\n",
      "                        of multiclass\r\n",
      "  --loss0 arg (=0, )    loss for correct label\r\n",
      "  --loss1 arg (=1, )    loss for incorrect label\r\n",
      "Make Multiclass into Warm-starting Contextual Bandit:\r\n",
      "  --warm_cb arg                        Convert multiclass on <k> classes into a\r\n",
      "                                       contextual bandit problem\r\n",
      "  --warm_cb_cs                         consume cost-sensitive classification \r\n",
      "                                       examples instead of multiclass\r\n",
      "  --loss0 arg (=0, )                   loss for correct label\r\n",
      "  --loss1 arg (=1, )                   loss for incorrect label\r\n",
      "  --warm_start arg (=0, )              number of training examples for warm \r\n",
      "                                       start phase\r\n",
      "  --epsilon arg                        epsilon-greedy exploration\r\n",
      "  --interaction arg (=4294967295, )    number of examples for the interactive \r\n",
      "                                       contextual bandit learning phase\r\n",
      "  --warm_start_update                  indicator of warm start updates\r\n",
      "  --interaction_update                 indicator of interaction updates\r\n",
      "  --corrupt_type_warm_start arg (=1, ) type of label corruption in the warm \r\n",
      "                                       start phase (1: uniformly at random, 2: \r\n",
      "                                       circular, 3: replacing with overwriting \r\n",
      "                                       label)\r\n",
      "  --corrupt_prob_warm_start arg (=0, ) probability of label corruption in the \r\n",
      "                                       warm start phase\r\n",
      "  --choices_lambda arg (=1, )          the number of candidate lambdas to \r\n",
      "                                       aggregate (lambda is the importance \r\n",
      "                                       weight parameter between the two \r\n",
      "                                       sources)\r\n",
      "  --lambda_scheme arg (=1, )           The scheme for generating candidate \r\n",
      "                                       lambda set (1: center lambda=0.5, 2: \r\n",
      "                                       center lambda=0.5, min lambda=0, max \r\n",
      "                                       lambda=1, 3: center lambda=epsilon/(1+ep\r\n",
      "                                       silon), 4: center lambda=epsilon/(1+epsi\r\n",
      "                                       lon), min lambda=0, max lambda=1); the \r\n",
      "                                       rest of candidate lambda values are \r\n",
      "                                       generated using a doubling scheme\r\n",
      "  --overwrite_label arg (=1, )         the label used by type 3 corruptions \r\n",
      "                                       (overwriting)\r\n",
      "  --sim_bandit                         simulate contextual bandit updates on \r\n",
      "                                       warm start examples\r\n",
      "Contextual Bandit Exploration with Action Dependent Features:\r\n",
      "  --cb_explore_adf          Online explore-exploit for a contextual bandit \r\n",
      "                            problem with multiline action dependent features\r\n",
      "  --first arg               tau-first exploration\r\n",
      "  --epsilon arg             epsilon-greedy exploration\r\n",
      "  --bag arg                 bagging-based exploration\r\n",
      "  --cover arg               Online cover based exploration\r\n",
      "  --psi arg (=1, )          disagreement parameter for cover\r\n",
      "  --nounif                  do not explore uniformly on zero-probability \r\n",
      "                            actions in cover\r\n",
      "  --softmax                 softmax exploration\r\n",
      "  --regcb                   RegCB-elim exploration\r\n",
      "  --regcbopt                RegCB optimistic exploration\r\n",
      "  --mellowness arg (=0.1, ) RegCB mellowness parameter c_0. Default 0.1\r\n",
      "  --greedify                always update first policy once in bagging\r\n",
      "  --cb_min_cost arg (=0, )  lower bound on cost\r\n",
      "  --cb_max_cost arg (=1, )  upper bound on cost\r\n",
      "  --first_only              Only explore the first action in a tie-breaking \r\n",
      "                            event\r\n",
      "  --lambda arg (=1, )       parameter for softmax\r\n",
      "  --cb_type arg             contextual bandit method to use in {ips,dr,mtr}. \r\n",
      "                            Default: mtr\r\n",
      "Contextual Bandit Exploration:\r\n",
      "  --cb_explore arg        Online explore-exploit for a <k> action contextual \r\n",
      "                          bandit problem\r\n",
      "  --first arg             tau-first exploration\r\n",
      "  --epsilon arg (=0.05, ) epsilon-greedy exploration\r\n",
      "  --bag arg               bagging-based exploration\r\n",
      "  --cover arg             Online cover based exploration\r\n",
      "  --psi arg (=1, )        disagreement parameter for cover\r\n",
      "Multiworld Testing Options:\r\n",
      "  --multiworld_test arg Evaluate features as a policies\r\n",
      "  --learn arg           Do Contextual Bandit learning on <n> classes.\r\n",
      "  --exclude_eval        Discard mwt policy features before learning\r\n",
      "Contextual Bandit with Action Dependent Features:\r\n",
      "  --cb_adf              Do Contextual Bandit learning with multiline action \r\n",
      "                        dependent features.\r\n",
      "  --rank_all            Return actions sorted by score order\r\n",
      "  --no_predict          Do not do a prediction when training\r\n",
      "  --cb_type arg         contextual bandit method to use in {ips, dm, dr, mtr, \r\n",
      "                        sm}. Default: mtr\r\n",
      "Contextual Bandit Options:\r\n",
      "  --cb arg              Use contextual bandit learning with <k> costs\r\n",
      "  --cb_type arg         contextual bandit method to use in {ips,dm,dr}\r\n",
      "  --eval                Evaluate a policy rather than optimizing.\r\n",
      "Cost Sensitive One Against All with Label Dependent Features:\r\n",
      "  --csoaa_ldf arg       Use one-against-all multiclass learning with label \r\n",
      "                        dependent features.\r\n",
      "  --ldf_override arg    Override singleline or multiline from csoaa_ldf or \r\n",
      "                        wap_ldf, eg if stored in file\r\n",
      "  --csoaa_rank          Return actions sorted by score order\r\n",
      "  --probabilities       predict probabilites of all classes\r\n",
      "Cost Sensitive One Against All with Label Dependent Features:\r\n",
      "  --wap_ldf arg         Use weighted all-pairs multiclass learning with label \r\n",
      "                        dependent features.  Specify singleline or multiline.\r\n",
      "Interact via elementwise multiplication:\r\n",
      "  --interact arg        Put weights on feature products from namespaces <n1> \r\n",
      "                        and <n2>\r\n",
      "Cost Sensitive One Against All:\r\n",
      "  --csoaa arg           One-against-all multiclass with <k> costs\r\n",
      "Cost-sensitive Active Learning:\r\n",
      "  --cs_active arg                       Cost-sensitive active learning with <k>\r\n",
      "                                        costs\r\n",
      "  --simulation                          cost-sensitive active learning \r\n",
      "                                        simulation mode\r\n",
      "  --baseline                            cost-sensitive active learning baseline\r\n",
      "  --domination arg (=1, )               cost-sensitive active learning use \r\n",
      "                                        domination. Default 1\r\n",
      "  --mellowness arg (=0.1, )             mellowness parameter c_0. Default 0.1.\r\n",
      "  --range_c arg (=0.5, )                parameter controlling the threshold for\r\n",
      "                                        per-label cost uncertainty. Default \r\n",
      "                                        0.5.\r\n",
      "  --max_labels arg (=18446744073709551615, )\r\n",
      "                                        maximum number of label queries.\r\n",
      "  --min_labels arg (=18446744073709551615, )\r\n",
      "                                        minimum number of label queries.\r\n",
      "  --cost_max arg (=1, )                 cost upper bound. Default 1.\r\n",
      "  --cost_min arg (=0, )                 cost lower bound. Default 0.\r\n",
      "  --csa_debug                           print debug stuff for cs_active\r\n",
      "Multilabel One Against All:\r\n",
      "  --multilabel_oaa arg  One-against-all multilabel with <k> labels\r\n",
      "importance weight classes:\r\n",
      "  --classweight arg     importance weight multiplier for class\r\n",
      "Memory Tree:\r\n",
      "  --memory_tree arg (=0, )             Make a memory tree with at most <n> \r\n",
      "                                       nodes\r\n",
      "  --max_number_of_labels arg (=10, )   max number of unique label\r\n",
      "  --leaf_example_multiplier arg (=1, ) multiplier on examples per leaf (default\r\n",
      "                                       = log nodes)\r\n",
      "  --alpha arg (=0.1, )                 Alpha\r\n",
      "  --dream_repeats arg (=1, )           number of dream operations per example \r\n",
      "                                       (default = 1)\r\n",
      "  --top_K arg (=1, )                   top K prediction error (default 1)\r\n",
      "  --learn_at_leaf                      whether or not learn at leaf (defualt = \r\n",
      "                                       True)\r\n",
      "  --oas                                use oas at the leaf\r\n",
      "  --dream_at_update arg (=0, )         turn on dream operations at reward based\r\n",
      "                                       update as well\r\n",
      "  --online                             turn on dream operations at reward based\r\n",
      "                                       update as well\r\n",
      "Recall Tree:\r\n",
      "  --recall_tree arg       Use online tree for multiclass\r\n",
      "  --max_candidates arg    maximum number of labels per leaf in the tree\r\n",
      "  --bern_hyper arg (=1, ) recall tree depth penalty\r\n",
      "  --max_depth arg         maximum depth of the tree, default log_2 (#classes)\r\n",
      "  --node_only             only use node features, not full path features\r\n",
      "  --randomized_routing    randomized routing\r\n",
      "Logarithmic Time Multiclass Tree:\r\n",
      "  --log_multi arg              Use online tree for multiclass\r\n",
      "  --no_progress                disable progressive validation\r\n",
      "  --swap_resistance arg (=4, ) disable progressive validation\r\n",
      "  --swap_resistance arg (=4, ) higher = more resistance to swap, default=4\r\n",
      "Error Correcting Tournament Options:\r\n",
      "  --ect arg                Error correcting tournament with <k> labels\r\n",
      "  --error arg (=0, )       errors allowed by ECT\r\n",
      "  --link arg (=identity, ) Specify the link function: identity, logistic, glf1 \r\n",
      "                           or poisson\r\n",
      "Boosting:\r\n",
      "  --boosting arg        Online boosting with <N> weak learners\r\n",
      "  --gamma arg (=0.1, )  weak learner's edge (=0.1), used only by online BBM\r\n",
      "  --alg arg (=BBM, )    specify the boosting algorithm: BBM (default), logistic\r\n",
      "                        (AdaBoost.OL.W), adaptive (AdaBoost.OL)\r\n",
      "One Against All Options:\r\n",
      "  --oaa arg             One-against-all multiclass with <k> labels\r\n",
      "  --oaa_subsample arg   subsample this number of negative examples when \r\n",
      "                        learning\r\n",
      "  --probabilities       predict probabilites of all classes\r\n",
      "  --scores              output raw scores per class\r\n",
      "Top K:\r\n",
      "  --top arg             top k recommendation\r\n",
      "Experience Replay:\r\n",
      "  --replay_m arg              use experience replay at a specified level \r\n",
      "                              [b=classification/regression, m=multiclass, \r\n",
      "                              c=cost sensitive] with specified buffer size\r\n",
      "  --replay_m_count arg (=1, ) how many times (in expectation) should each \r\n",
      "                              example be played (default: 1 = permuting)\r\n",
      "Binary loss:\r\n",
      "  --binary              report loss as binary classification on -1,1\r\n",
      "Bootstrap:\r\n",
      "  --bootstrap arg       k-way bootstrap by online importance resampling\r\n",
      "  --bs_type arg         prediction type {mean,vote}\r\n",
      "scorer options:\r\n",
      "  --link arg (=identity, ) Specify the link function: identity, logistic, glf1 \r\n",
      "                           or poisson\r\n",
      "Stagewise polynomial options:\r\n",
      "  --stage_poly                use stagewise polynomial feature learning\r\n",
      "  --sched_exponent arg (=1, ) exponent controlling quantity of included \r\n",
      "                              features\r\n",
      "  --batch_sz arg (=1000, )    multiplier on batch size before including more \r\n",
      "                              features\r\n",
      "  --batch_sz_no_doubling      batch_sz does not double\r\n",
      "Low Rank Quadratics FA:\r\n",
      "  --lrqfa arg           use low rank quadratic features with field aware \r\n",
      "                        weights\r\n",
      "Low Rank Quadratics:\r\n",
      "  --lrq arg             use low rank quadratic features\r\n",
      "  --lrqdropout          use dropout training for low rank quadratic features\r\n",
      "Autolink:\r\n",
      "  --autolink arg        create link function with polynomial d\r\n",
      "VW options:\r\n",
      "  --marginal arg                   substitute marginal label estimates for ids\r\n",
      "  --initial_denominator arg (=1, ) initial denominator\r\n",
      "  --initial_numerator arg (=0.5, ) initial numerator\r\n",
      "  --compete                        enable competition with marginal features\r\n",
      "  --update_before_learn            update marginal values before learning\r\n",
      "  --unweighted_marginals           ignore importance weights when computing \r\n",
      "                                   marginals\r\n",
      "  --decay arg (=0, )               decay multiplier per event (1e-3 for \r\n",
      "                                   example)\r\n",
      "Matrix Factorization Reduction:\r\n",
      "  --new_mf arg          rank for reduction-based matrix factorization\r\n",
      "Neural Network:\r\n",
      "  --nn arg              Sigmoidal feedforward network with <k> hidden units\r\n",
      "  --inpass              Train or test sigmoidal feedforward network with input \r\n",
      "                        passthrough.\r\n",
      "  --multitask           Share hidden layer across all reduced tasks.\r\n",
      "  --dropout             Train or test sigmoidal feedforward network using \r\n",
      "                        dropout.\r\n",
      "  --meanfield           Train or test sigmoidal feedforward network using mean \r\n",
      "                        field.\r\n",
      "Confidence:\r\n",
      "  --confidence                 Get confidence for binary predictions\r\n",
      "  --confidence_after_training  Confidence after training\r\n",
      "Active Learning with Cover:\r\n",
      "  --active_cover                enable active learning with cover\r\n",
      "  --mellowness arg (=8, )       active learning mellowness parameter c_0. \r\n",
      "                                Default 8.\r\n",
      "  --alpha arg (=1, )            active learning variance upper bound parameter \r\n",
      "                                alpha. Default 1.\r\n",
      "  --beta_scale arg (=3.16228, ) active learning variance upper bound parameter \r\n",
      "                                beta_scale. Default sqrt(10).\r\n",
      "  --cover arg (=12, )           cover size. Default 12.\r\n",
      "  --oracular                    Use Oracular-CAL style query or not. Default \r\n",
      "                                false.\r\n",
      "Active Learning:\r\n",
      "  --active                enable active learning\r\n",
      "  --simulation            active learning simulation mode\r\n",
      "  --mellowness arg (=8, ) active learning mellowness parameter c_0. Default 8\r\n",
      "Experience Replay:\r\n",
      "  --replay_b arg              use experience replay at a specified level \r\n",
      "                              [b=classification/regression, m=multiclass, \r\n",
      "                              c=cost sensitive] with specified buffer size\r\n",
      "  --replay_b_count arg (=1, ) how many times (in expectation) should each \r\n",
      "                              example be played (default: 1 = permuting)\r\n",
      "Baseline options:\r\n",
      "  --baseline            Learn an additive baseline (from constant features) and\r\n",
      "                        a residual separately in regression.\r\n",
      "  --lr_multiplier arg   learning rate multiplier for baseline model\r\n",
      "  --global_only         use separate example with only global constant for \r\n",
      "                        baseline predictions\r\n",
      "  --check_enabled       only use baseline when the example contains enabled \r\n",
      "                        flag\r\n",
      "OjaNewton options:\r\n",
      "  --OjaNewton                    Online Newton with Oja's Sketch\r\n",
      "  --sketch_size arg (=10, )      size of sketch\r\n",
      "  --epoch_size arg (=1, )        size of epoch\r\n",
      "  --alpha arg (=1, )             mutiplicative constant for indentiy\r\n",
      "  --alpha_inverse arg            one over alpha, similar to learning rate\r\n",
      "  --learning_rate_cnt arg (=2, ) constant for the learning rate 1/t\r\n",
      "  --normalize arg                normalize the features or not\r\n",
      "  --random_init arg              randomize initialization of Oja or not\r\n",
      "LBFGS and Conjugate Gradient options:\r\n",
      "  --conjugate_gradient  use conjugate gradient based optimization\r\n",
      "LBFGS and Conjugate Gradient options:\r\n",
      "  --bfgs                       use conjugate gradient based optimization\r\n",
      "  --hessian_on                 use second derivative in line search\r\n",
      "  --mem arg (=15, )            memory in bfgs\r\n",
      "  --termination arg (=0.001, ) Termination threshold\r\n",
      "Latent Dirichlet Allocation:\r\n",
      "  --lda arg                    Run lda with <int> topics\r\n",
      "  --lda_alpha arg (=0.1, )     Prior on sparsity of per-document topic weights\r\n",
      "  --lda_rho arg (=0.1, )       Prior on sparsity of topic distributions\r\n",
      "  --lda_D arg (=10000, )       Number of documents\r\n",
      "  --lda_epsilon arg (=0.001, ) Loop convergence threshold\r\n",
      "  --minibatch arg (=1, )       Minibatch size, for LDA\r\n",
      "  --math-mode arg (=0, )       Math mode: simd, accuracy, fast-approx\r\n",
      "  --metrics                    Compute metrics\r\n",
      "Noop Learner:\r\n",
      "  --noop                do no learning\r\n",
      "Print psuedolearner:\r\n",
      "  --print               print examples\r\n",
      "Gradient Descent Matrix Factorization:\r\n",
      "  --rank arg            rank for matrix factorization.\r\n",
      "  --bfgs                Option not supported by this reduction\r\n",
      "  --conjugate_gradient  Option not supported by this reduction\r\n",
      "Network sending:\r\n",
      "  --sendto arg          send examples to <host>\r\n",
      "Stochastic Variance Reduced Gradient:\r\n",
      "  --svrg                  Streaming Stochastic Variance Reduced Gradient\r\n",
      "  --stage_size arg (=1, ) Number of passes per SVRG stage\r\n",
      "Follow the Regularized Leader:\r\n",
      "  --ftrl                FTRL: Follow the Proximal Regularized Leader\r\n",
      "  --coin                Coin betting optimizer\r\n",
      "  --pistol              PiSTOL: Parameter-free STOchastic Learning\r\n",
      "  --ftrl_alpha arg      Learning rate for FTRL optimization\r\n",
      "  --ftrl_beta arg       Learning rate for FTRL optimization\r\n",
      "Kernel SVM:\r\n",
      "  --ksvm                   kernel svm\r\n",
      "  --reprocess arg (=1, )   number of reprocess steps for LASVM\r\n",
      "  --pool_greedy            use greedy selection on mini pools\r\n",
      "  --para_active            do parallel active learning\r\n",
      "  --pool_size arg (=1, )   size of pools for active learning\r\n",
      "  --subsample arg (=1, )   number of items to subsample from the pool\r\n",
      "  --kernel arg (=linear, ) type of kernel (rbf or linear (default))\r\n",
      "  --bandwidth arg (=1, )   bandwidth of rbf kernel\r\n",
      "  --degree arg (=2, )      degree of poly kernel\r\n",
      "Gradient Descent options:\r\n",
      "  --sgd                  use regular stochastic gradient descent update.\r\n",
      "  --adaptive             use adaptive, individual learning rates.\r\n",
      "  --adax                 use adaptive learning rates with x^2 instead of g^2x^2\r\n",
      "  --invariant            use safe/importance aware updates.\r\n",
      "  --normalized           use per feature normalized updates\r\n",
      "  --sparse_l2 arg (=0, ) use per feature normalized updates\r\n",
      "  --l1_state arg (=0, )  use per feature normalized updates\r\n",
      "  --l2_state arg (=1, )  use per feature normalized updates\r\n",
      "Input options:\r\n",
      "  -d [ --data ] arg     Example set\r\n",
      "  --daemon              persistent daemon mode on port 26542\r\n",
      "  --foreground          in persistent daemon mode, do not run in the background\r\n",
      "  --port arg            port to listen on; use 0 to pick unused port\r\n",
      "  --num_children arg    number of children for persistent daemon mode\r\n",
      "  --pid_file arg        Write pid file in persistent daemon mode\r\n",
      "  --port_file arg       Write port used in persistent daemon mode\r\n",
      "  -c [ --cache ]        Use a cache.  The default is <data>.cache\r\n",
      "  --cache_file arg      The location(s) of cache_file.\r\n",
      "  --json                Enable JSON parsing.\r\n",
      "  --dsjson              Enable Decision Service JSON parsing.\r\n",
      "  -k [ --kill_cache ]   do not reuse existing cache: create a new one always\r\n",
      "  --compressed          use gzip format whenever possible. If a cache file is \r\n",
      "                        being created, this option creates a compressed cache \r\n",
      "                        file. A mixture of raw-text & compressed inputs are \r\n",
      "                        supported with autodetection.\r\n",
      "  --no_stdin            do not default to reading from stdin\r\n"
     ]
    }
   ],
   "source": [
    "! ./vw -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = /mnt/predictions.txt\n",
      "Enabling FTRL based optimization\n",
      "Algorithm used: Proximal-FTRL\n",
      "ftrl_alpha = 0.005\n",
      "ftrl_beta = 0.1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = /mnt/test.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "warning: ? is not a good float, replacing with 0\n",
      "warning: ? is not a good float, replacing with 0\n",
      "\n",
      "finished run\n",
      "number of examples = 2\n",
      "weighted example sum = 2.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 5.665224\n",
      "total feature number = 26\n",
      "0.225489 tag1\n",
      "0.041817 tag2\n"
     ]
    }
   ],
   "source": [
    "# make prediction with VW\n",
    "! echo \"? tag1| week_of_day_3 country_US state_CA dma_807 ad_document_id_132815_1227645 display_document_id_10000108_1227645 display_id_10000108 ad_id_132815 document_id_1227645 campaign_id_17018 advertiser_id_331 platform_1\" > /mnt/test.txt\n",
    "! echo \"? tag2| week_of_day_3 country_US state_CA dma_807 ad_document_id_133677_1297868 display_document_id_10000108_1297868 display_id_10000108 ad_id_133677 document_id_1297868 campaign_id_17143 advertiser_id_1919 platform_1\" >> /mnt/test.txt\n",
    "! ./vw -d /mnt/test.txt -i model -t -k -p /mnt/predictions.txt --progress 1000000 --link=logistic\n",
    "# predicted probabilities of \"1\" class\n",
    "! cat /mnt/predictions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [   
    "\n",
    "Train a baseline model using the following features:\n",
    "- **clicked**\n",
    "- geo_location features (country, state, dma)\n",
    "- day_of_week (from timestamp, use *date.isoweekday()*)\n",
    "- ad_id\n",
    "- campaign_id\n",
    "- advertiser_id\n",
    "- ad_document_id\n",
    "- display_document_id\n",
    "- platform\n",
    "\n",
    "Make submission to Kaggle to know your leaderboard score\n",    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [    
   ],
   "source": [
    "%%time\n",
    "se.sql(\"\"\"\n",
    "select \n",
    "    clicks_train.clicked,\n",
    "    clicks_train.display_id,\n",
    "    clicks_train.ad_id,\n",
    "    promoted_content.document_id,\n",
    "    promoted_content.campaign_id,\n",
    "    promoted_content.advertiser_id\n",
    "from clicks_train join promoted_content on clicks_train.ad_id = promoted_content.ad_id\n",
    "\"\"\").write.parquet(\"/train_features.parquet\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se.read.parquet(\"/train_features.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! hdfs dfs -rm -r /train_features.txt\n",
    "(\n",
    "    se.read.parquet(\"/train_features.parquet\")\n",
    "    .rdd\n",
    "    .map(vw_row_mapper)\n",
    "    .saveAsTextFile(\"/train_features.txt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -du -s -h /train_features.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# copy file to local master node\n",
    "! rm /mnt/train.txt\n",
    "! hdfs dfs -getmerge /train_features.txt /mnt/train.txt\n",
    "# preview local file\n",
    "! head -n 5 /mnt/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./vw -d /mnt/train.txt -b 24 -c -k --ftrl --passes 1 -f model --holdout_off --loss_function logistic --random_seed 42 --progress 8000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction with VW\n",
    "! echo \"? tag1| ad_id_144739 document_id_1337362 campaign_id_18488 advertiser_id_2909\" > /mnt/test.txt\n",
    "! echo \"? tag2| ad_id_156824 document_id_992370 campaign_id_7283 advertiser_id_1919\" >> /mnt/test.txt\n",
    "! ./vw -d /mnt/test.txt -i model -t -k -p /mnt/predictions.txt --progress 1000000 --link=logistic\n",
    "# predicted probabilities of \"1\" class\n",
    "! cat /mnt/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting to Kaggle\n",
    "\n",
    "Obtain Kaggle API token: https://github.com/Kaggle/kaggle-api#api-credentials\n",
    "\n",
    "Making a submission: https://github.com/Kaggle/kaggle-api#submit-to-a-competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"username\":\"****\",\"key\":\"*****\"}\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! touch ~/.kaggle/kaggle.json\n",
    "! echo '{\"username\":\"yosibekker\",\"key\":\"9ed1c7bdfc66775c0378c8445eb54b55\"}' > ~/.kaggle/kaggle.json\n",
    "! cat ~/.kaggle/kaggle.json\n",
    "! chmod 600 /home/hadoop/.kaggle/kaggle.json"
   ]
  },  
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_id,ad_id\r\n",
      "16874594,66758 150083 162754 170392 172888 180797\r\n",
      "16874595,8846 30609 143982\r\n",
      "16874596,11430 57197 132820 153260 173005 288385 289122 289915\r\n",
      "16874597,137858 143981 155945 180965 182039 285834 305790 308836\r\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/c/outbrain-click-prediction/overview/evaluation\n",
    "# For each display_id in the test set, you must predict a space-delimited list of ad_ids, \n",
    "# ordered by decreasing likelihood of being clicked.\n",
    "! head -n 5 ./sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.92 ms, sys: 0 ns, total: 4.92 ms\n",
      "Wall time: 39.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "se.sql(\"\"\"\n",
    "select \n",
    "    \"0\" as clicked,\n",
    "     date_format(( cast(current_timestamp as TIMESTAMP) + INTERVAL 1465876799 seconds), \"u\") as week_of_day,        \n",
    "    split(e.geo_location, '>')[0] as country,\n",
    "    CASE WHEN size(split(e.geo_location, '>')) >= 2 THEN split(e.geo_location, '>')[1] \n",
    "    ELSE ''\n",
    "    END as state,\n",
    "    CASE WHEN size(split(e.geo_location, '>')) >= 3 THEN split(e.geo_location, '>')[2] \n",
    "    ELSE ''\n",
    "    END as dma,\n",
    "    CONCAT(pc.ad_id, '_', pc.document_id) AS ad_document_id,\n",
    "    CONCAT(ct.display_id, '_', pc.document_id) AS display_document_id,    \n",
    "    ct.display_id,\n",
    "    ct.ad_id,\n",
    "    pc.document_id,\n",
    "    pc.campaign_id,\n",
    "    pc.advertiser_id,\n",
    "    e.platform\n",
    "from clicks_test ct, events e, promoted_content pc where ct.ad_id == pc.ad_id and e.display_id == ct.display_id\n",
    "\"\"\").write.parquet(\"/test_features.parquet\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/test_features.txt': No such file or directory\n",
      "CPU times: user 53 ms, sys: 19.8 ms, total: 72.7 ms\n",
      "Wall time: 35.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! hdfs dfs -rm -r /test_features.txt\n",
    "(\n",
    "    se.read.parquet(\"/test_features.parquet\")\n",
    "    .rdd\n",
    "    .map(vw_row_mapper)\n",
    "    .saveAsTextFile(\"/test_features.txt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 16874673_91797| week_of_day_3 country_US state_OH dma_542 ad_document_id_91797_1037379 display_document_id_16874673_1037379 display_id_16874673 ad_id_91797 document_id_1037379 campaign_id_11888 advertiser_id_1347 platform_1\r\n",
      "-1 16874673_107055| week_of_day_3 country_US state_OH dma_542 ad_document_id_107055_208091 display_document_id_16874673_208091 display_id_16874673 ad_id_107055 document_id_208091 campaign_id_13829 advertiser_id_372 platform_1\r\n",
      "-1 16874673_158923| week_of_day_3 country_US state_OH dma_542 ad_document_id_158923_647796 display_document_id_16874673_647796 display_id_16874673 ad_id_158923 document_id_647796 campaign_id_2236 advertiser_id_1309 platform_1\r\n",
      "-1 16874673_207162| week_of_day_3 country_US state_OH dma_542 ad_document_id_207162_1367996 display_document_id_16874673_1367996 display_id_16874673 ad_id_207162 document_id_1367996 campaign_id_23856 advertiser_id_17 platform_1\r\n",
      "-1 16874705_140897| week_of_day_3 country_CA state_BC dma_ ad_document_id_140897_1322901 display_document_id_16874705_1322901 display_id_16874705 ad_id_140897 document_id_1322901 campaign_id_6202 advertiser_id_83 platform_2\r\n"
     ]
    }
   ],
   "source": [
    "# copy file to local master node\n",
    "! rm /mnt/test.txt\n",
    "! hdfs dfs -getmerge /test_features.txt /mnt/test.txt\n",
    "# preview local file\n",
    "! head -n 5 /mnt/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = /mnt/predictions.txt\n",
      "Enabling FTRL based optimization\n",
      "Algorithm used: Proximal-FTRL\n",
      "ftrl_alpha = 0.005\n",
      "ftrl_beta = 0.1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = /mnt/test.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.956585 0.956585      1000000      1000000.0  -1.0000   0.1907       13\n",
      "0.955657 0.954730      2000000      2000000.0  -1.0000   0.0341       13\n",
      "0.953847 0.950225      3000000      3000000.0  -1.0000   0.4832       13\n",
      "0.953116 0.950922      4000000      4000000.0  -1.0000   0.1908       13\n",
      "0.953312 0.954098      5000000      5000000.0  -1.0000   0.2701       13\n",
      "0.953068 0.951847      6000000      6000000.0  -1.0000   0.1202       13\n",
      "0.953004 0.952623      7000000      7000000.0  -1.0000   0.2185       13\n",
      "0.952714 0.950683      8000000      8000000.0  -1.0000   0.2511       13\n",
      "0.952444 0.950279      9000000      9000000.0  -1.0000   0.2479       13\n",
      "0.952516 0.953167     10000000     10000000.0  -1.0000   0.1636       13\n",
      "0.952490 0.952226     11000000     11000000.0  -1.0000   0.2168       13\n",
      "0.952712 0.955156     12000000     12000000.0  -1.0000   0.5446       13\n",
      "0.952528 0.950319     13000000     13000000.0  -1.0000   0.2012       13\n",
      "0.952554 0.952891     14000000     14000000.0  -1.0000   0.2861       13\n",
      "0.952521 0.952067     15000000     15000000.0  -1.0000   0.3005       13\n",
      "0.952538 0.952791     16000000     16000000.0  -1.0000   0.1633       13\n",
      "0.952506 0.951990     17000000     17000000.0  -1.0000   0.1191       13\n",
      "0.952338 0.949486     18000000     18000000.0  -1.0000   0.0684       13\n",
      "0.952148 0.948719     19000000     19000000.0  -1.0000   0.1941       13\n",
      "0.952056 0.950315     20000000     20000000.0  -1.0000   0.1805       13\n",
      "0.952065 0.952240     21000000     21000000.0  -1.0000   0.2099       13\n",
      "0.952077 0.952332     22000000     22000000.0  -1.0000   0.1534       13\n",
      "0.952120 0.953065     23000000     23000000.0  -1.0000   0.1659       13\n",
      "0.951981 0.948796     24000000     24000000.0  -1.0000   0.2223       13\n",
      "0.952049 0.953669     25000000     25000000.0  -1.0000   0.1157       13\n",
      "0.952041 0.951843     26000000     26000000.0  -1.0000   0.2463       13\n",
      "0.952089 0.953342     27000000     27000000.0  -1.0000   0.1250       13\n",
      "0.952194 0.955039     28000000     28000000.0  -1.0000   0.4537       13\n",
      "0.952007 0.946766     29000000     29000000.0  -1.0000   0.1826       13\n",
      "0.952099 0.954757     30000000     30000000.0  -1.0000   0.1968       13\n",
      "0.952112 0.952499     31000000     31000000.0  -1.0000   0.0672       13\n",
      "0.952152 0.953383     32000000     32000000.0  -1.0000   0.0631       13\n",
      "\n",
      "finished run\n",
      "number of examples = 32225162\n",
      "weighted example sum = 32225162.000000\n",
      "weighted label sum = -32225162.000000\n",
      "average loss = 0.952145\n",
      "best constant = -1.000000\n",
      "best constant's loss = 0.000000\n",
      "total feature number = 418927106\n",
      "0.225425 16874673_91797\n",
      "0.098729 16874673_107055\n",
      "0.058896 16874673_158923\n",
      "0.198974 16874673_207162\n",
      "0.236702 16874705_140897\n"
     ]
    }
   ],
   "source": [
    "! ./vw -d /mnt/test.txt -i model -t -k -p /mnt/predictions.txt --progress 1000000 --link=logistic\n",
    "# predicted probabilities of \"1\" class\n",
    "! head -n 5 /mnt/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32225162 /mnt/predictions.txt\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l /mnt/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19ef3a79a7e4d25909b7567bf85e9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "scores_by_display_id = defaultdict(dict)\n",
    "for line in tqdm.tqdm(open('/mnt/predictions.txt')):\n",
    "    score, tag = line.strip().split(\" \")\n",
    "    score = float(score)\n",
    "    display_id, ad_id = tag.split(\"_\")\n",
    "    scores_by_display_id[display_id][ad_id] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7ba45a15514f8daca31bf6aa1d477b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6245533.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"submission.txt\", \"w\") as f:\n",
    "    f.write(\"display_id,ad_id\\n\")\n",
    "    for k, vs in tqdm.tqdm_notebook(scores_by_display_id.items()):\n",
    "        f.write(\"{},{}\\n\".format(\n",
    "            k, \n",
    "            \" \".join([v[0] for v in sorted(vs.items(), key=lambda x: -x[1])])\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|########################################| 260M/260M [00:03<00:00, 69.4MB/s]\n",
      "Successfully submitted to Outbrain Click Prediction"
     ]
    }
   ],
   "source": [
    "! kaggle competitions submit -f submission.txt outbrain-click-prediction -m \"LR_Baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
